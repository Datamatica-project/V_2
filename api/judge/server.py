from __future__ import annotations

import os
import time
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

import yaml
import requests
from fastapi import BackgroundTasks, FastAPI, HTTPException, Body, Path as FPath, Query
from pydantic import BaseModel, Field

from src.judge.orchestrator.run_batch import run_batch
from src.judge.orchestrator.status import read_status
from src.common.utils.logging import setup_logger, LogContext, log_event
from src.common.dto.dto import GTTrainRequest, GTTrainResponse

from api.routers.gt_register import router as gt_router
from api.routers.unlabeled_register import router as unlabeled_router

# -----------------------------------------------------------------------------
# Utils
# -----------------------------------------------------------------------------
def _env_path(name: str, default: str) -> Path:
    return Path(os.getenv(name, default)).resolve()


CONFIGS_DIR = _env_path("CONFIGS_DIR", "/workspace/configs")
FALLBACK_CONFIGS_DIR = Path(__file__).resolve().parent


def _load_yaml(path: Path) -> Dict[str, Any]:
    if not path.exists():
        raise FileNotFoundError(str(path))
    raw = path.read_text(encoding="utf-8")
    try:
        data = yaml.safe_load(raw) or {}
    except Exception as e:
        raise ValueError(f"invalid yaml: {path} ({e})") from e
    if not isinstance(data, dict):
        raise ValueError(f"yaml root must be a mapping(dict): {path}")
    return data


# -----------------------------------------------------------------------------
# models.yaml loader
# -----------------------------------------------------------------------------
def _pick_models_yaml_path() -> Path:
    p = CONFIGS_DIR / "models.yaml"
    if p.exists():
        return p
    return FALLBACK_CONFIGS_DIR / "models.yaml"


def _load_models_cfg_with_path() -> Tuple[Dict[str, Any], Path]:
    p = _pick_models_yaml_path()
    return _load_yaml(p), p


def _validate_models_cfg(models_cfg: Dict[str, Any]) -> None:
    if "models" not in models_cfg:
        raise ValueError("models.yaml missing 'models'")

    for name in ("yolov11", "rtm", "rtdetr"):
        m = models_cfg["models"].get(name)
        if not isinstance(m, dict):
            raise ValueError(f"models.yaml missing models.{name}")
        if not m.get("base_url"):
            raise ValueError(f"models.yaml missing models.{name}.base_url")


def _build_gateway_model_cfg(models_cfg: Dict[str, Any]) -> Dict[str, Any]:
    out: Dict[str, Any] = {"models": {}}
    for name in ("yolov11", "rtm", "rtdetr"):
        m = models_cfg["models"][name]
        base_url = m["base_url"].rstrip("/")
        infer_ep = m.get("endpoints", {}).get("infer", "/infer")
        out["models"][name] = {"url": f"{base_url}{infer_ep}"}
    return out


# -----------------------------------------------------------------------------
# ensemble.yaml loader
# -----------------------------------------------------------------------------
def _pick_ensemble_yaml_path() -> Path:
    p = CONFIGS_DIR / "ensemble.yaml"
    if p.exists():
        return p
    return FALLBACK_CONFIGS_DIR / "ensemble.yaml"


def _load_ensemble_cfg_with_path() -> Tuple[Dict[str, Any], Path]:
    p = _pick_ensemble_yaml_path()
    return _load_yaml(p), p


# -----------------------------------------------------------------------------
# Infer DTO
# -----------------------------------------------------------------------------
class InferRunRequest(BaseModel):
    run_id: Optional[str] = Field(
        default=None,
        description="(ì„ íƒ) ì™¸ë¶€ì—ì„œ ì§€ì •í•˜ëŠ” run_id. ë¯¸ì§€ì • ì‹œ ì„œë²„ê°€ run_YYYYmmdd_HHMMSS í˜•íƒœë¡œ ìë™ ìƒì„±",
        examples=["run_20260205_141500"],
    )

    user_id: Optional[str] = Field(
        default=None,
        description=(
            "(ì„ íƒ) í”„ë¡œì íŠ¸ ìŠ¤ì½”í”„ìš© ì‚¬ìš©ì í‚¤. "
            "ì œê³µ ì‹œ ëª¨ë¸ ì»¨í…Œì´ë„ˆì—ì„œ projects/{user}/{project}/{model} weight ìŠ¤ì½”í”„ ì„ íƒ ë“±ì— ì‚¬ìš© ê°€ëŠ¥"
        ),
        examples=["user_001"],
    )
    project_id: Optional[str] = Field(
        default=None,
        description="(ì„ íƒ) í”„ë¡œì íŠ¸ ìŠ¤ì½”í”„ìš© í”„ë¡œì íŠ¸ í‚¤",
        examples=["project_demo"],
    )

    unlabeled_dir: str = Field(
        ...,
        description="ë¼ë²¨ ì—†ëŠ” ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬(ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ). ì´ ê²½ë¡œë¥¼ ìŠ¤ìº”í•´ ì¶”ë¡  ëŒ€ìƒ ì´ë¯¸ì§€ë¥¼ ìˆ˜ì§‘",
        examples=["/workspace/storage/datasets/unlabeled/images"],
    )

    batch_size: int = Field(
        8,
        ge=1,
        le=4096,
        description="ë°°ì¹˜ë‹¹ ì´ë¯¸ì§€ ê°œìˆ˜. ë„ˆë¬´ í¬ë©´ VRAM OOM/timeout/ë„¤íŠ¸ì›Œí¬ ë³‘ëª© ê°€ëŠ¥",
        examples=[8, 16],
    )

    segment_size_batches: int = Field(
        100,
        ge=1,
        le=100000,
        description="ê²°ê³¼ë¥¼ segment ë‹¨ìœ„ë¡œ ì €ì¥í•  ë•Œ segmentì— í¬í•¨ë  ë°°ì¹˜ ìˆ˜(ë¡œê·¸/ì €ì¥ íŒŒì¼ ë¶„í• ìš©)",
        examples=[100],
    )

    infer_params: Dict[str, Any] = Field(
        default_factory=dict,
        description=(
            "ëª¨ë¸ infer APIë¡œ ì „ë‹¬í•  ì¶”ê°€ íŒŒë¼ë¯¸í„°.\n"
            "- ê³µí†µ ì˜ˆ: imgsz, conf, device\n"
            "- YOLO ì „ìš© ì˜ˆ: iou\n"
            "- (ì„ íƒ) weight/weight_path, weight_dir(ìŠ¤ì½”í”„)\n"
            "â€» run_id/user_id/project_idëŠ” ì„œë²„ê°€ infer_paramsì— ìë™ ë³‘í•©(setdefault)ë  ìˆ˜ ìˆìŒ"
        ),
        examples=[{"conf": 0.25, "iou": 0.45, "imgsz": 640, "device": "0"}],
    )

    model_config = {
        "extra": "ignore",
        "json_schema_extra": {
            "examples": [
                {
                    "run_id": "run_20260205_141500",
                    "user_id": "user_001",
                    "project_id": "project_demo",
                    "unlabeled_dir": "/workspace/storage/datasets/unlabeled/images",
                    "batch_size": 8,
                    "segment_size_batches": 100,
                    "infer_params": {"conf": 0.25, "iou": 0.45, "imgsz": 640, "device": "0"},
                }
            ]
        },
    }


class InferRunResponse(BaseModel):
    ok: bool = True
    run_id: str
    status: str


# -----------------------------------------------------------------------------
# App & Logger (Swagger ë¬¸ì„œí™” ê°•í™”)
# -----------------------------------------------------------------------------
app = FastAPI(
    title="V2 Judge (Control Plane)",
    version="0.3.0",
    openapi_tags=[
        {"name": "Health", "description": "ì„œë²„ ìƒíƒœ í™•ì¸(í—¬ìŠ¤ì²´í¬)"},
        {
            "name": "Loop: Inference",
            "description": (
                "Unlabeled ì´ë¯¸ì§€ì— ëŒ€í•´ ë°°ì¹˜ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ê³ (3ëª¨ë¸ fan-out), "
                "Judgeì—ì„œ ì•™ìƒë¸”(PASS_3/PASS_2/FAIL/MISS) ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” ë¹„ë™ê¸° ë£¨í”„"
            ),
        },
        {
            "name": "Loop: Train GT",
            "description": (
                "GT í•™ìŠµì„ ëª¨ë¸ ì»¨í…Œì´ë„ˆë¡œ ìœ„ì„í•˜ì—¬ íŠ¸ë¦¬ê±°/ìƒíƒœì¡°íšŒ í•˜ëŠ” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ API. "
                "ë‹¨ì¼ ëª¨ë¸ ì‹¤í–‰ ë˜ëŠ” 3ëª¨ë¸ ì „ì²´ ì‹¤í–‰ì„ ì§€ì›"
            ),
        },
        {"name": "Debug", "description": "ì„¤ì • íŒŒì¼(models.yaml / ensemble.yaml) ë¡œë”© ìƒíƒœ í™•ì¸"},
    ],
)

app.include_router(gt_router)
app.include_router(unlabeled_router)

logger = setup_logger(
    service="judge",
    log_file=os.getenv("JUDGE_API_LOG_FILE", "/workspace/logs/judge/api.jsonl"),
    level=os.getenv("LOG_LEVEL", "INFO"),
)


# -----------------------------------------------------------------------------
# Health
# -----------------------------------------------------------------------------
@app.get(
    "/health",
    tags=["Health"],
    summary="í—¬ìŠ¤ì²´í¬",
    description="Judge(Control Plane) ì„œë¹„ìŠ¤ê°€ ì •ìƒ ë™ì‘ ì¤‘ì¸ì§€ í™•ì¸í•©ë‹ˆë‹¤.",
    responses={200: {"description": "ì •ìƒ ë™ì‘"}},
)
def health():
    return {"ok": True}


# -----------------------------------------------------------------------------
# (ì„ íƒ) Debug: configs í™•ì¸ (Swaggerì— ë„ì›€ë¨)
# -----------------------------------------------------------------------------
@app.get(
    "/debug/config/models",
    tags=["Debug"],
    summary="models.yaml ë¡œë”©/ì •ê·œí™” ê²°ê³¼ í™•ì¸",
    description=(
        "Judgeê°€ ì‹¤ì œë¡œ ì‚¬ìš© ì¤‘ì¸ models.yaml ê²½ë¡œì™€, "
        "ëª¨ë¸ë³„ infer URLë¡œ ì •ê·œí™”ëœ ì„¤ì •ì„ í™•ì¸í•©ë‹ˆë‹¤."
    ),
)
def debug_models_config():
    cfg, path = _load_models_cfg_with_path()
    _validate_models_cfg(cfg)
    return {
        "ok": True,
        "models_yaml_path": str(path),
        "configs_dir": str(CONFIGS_DIR),
        "fallback_dir": str(FALLBACK_CONFIGS_DIR),
        "gateway_model_cfg": _build_gateway_model_cfg(cfg),
    }


@app.get(
    "/debug/config/ensemble",
    tags=["Debug"],
    summary="ensemble.yaml ë¡œë”© ê²°ê³¼ í™•ì¸",
    description="Judgeê°€ ì‹¤ì œë¡œ ì‚¬ìš© ì¤‘ì¸ ensemble.yaml ê²½ë¡œì™€ ë¡œë”©ëœ ì„¤ì •ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
)
def debug_ensemble_config():
    cfg, path = _load_ensemble_cfg_with_path()
    return {
        "ok": True,
        "ensemble_yaml_path": str(path),
        "configs_dir": str(CONFIGS_DIR),
        "fallback_dir": str(FALLBACK_CONFIGS_DIR),
        "ensemble_cfg": cfg,
    }


# -----------------------------------------------------------------------------
# Infer (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
# -----------------------------------------------------------------------------
def _merge_infer_params(req: InferRunRequest) -> Dict[str, Any]:
    p = dict(req.infer_params or {})
    if req.run_id:
        p.setdefault("run_id", req.run_id)
    if req.user_id:
        p.setdefault("user_id", req.user_id)
    if req.project_id:
        p.setdefault("project_id", req.project_id)
    return p


def _do_infer(req: InferRunRequest) -> None:
    ctx = LogContext(run_id=req.run_id, stage="infer", model="judge")

    models_cfg, _ = _load_models_cfg_with_path()
    model_cfg = _build_gateway_model_cfg(models_cfg)

    ensemble_cfg, _ = _load_ensemble_cfg_with_path()

    log_event(
        logger,
        "loop infer background task start",
        ctx=ctx,
        unlabeled_dir=req.unlabeled_dir,
        batch_size=req.batch_size,
        segment_size_batches=req.segment_size_batches,
        infer_params_keys=sorted(list((_merge_infer_params(req) or {}).keys())),
    )

    run_batch(
        run_id=req.run_id,
        unlabeled_dir=req.unlabeled_dir,
        batch_size=req.batch_size,
        model_cfg=model_cfg,
        infer_params=_merge_infer_params(req),
        segment_size_batches=req.segment_size_batches,
        ensemble_cfg=ensemble_cfg,
    )

    log_event(logger, "loop infer background task done", ctx=ctx)


@app.post(
    "/loop/infer/run",
    response_model=InferRunResponse,
    tags=["Loop: Inference"],
    summary="Unlabeled ì¶”ë¡ +ì•™ìƒë¸” ë£¨í”„ ì‹œì‘",
    description=(
        "unlabeled_dir ë‚´ ì´ë¯¸ì§€ë¥¼ ìŠ¤ìº”í•œ ë’¤ batch_size ë‹¨ìœ„ë¡œ 3ê°œ ëª¨ë¸ì— ì¶”ë¡ ì„ ìš”ì²­í•˜ê³ , "
        "Judgeì—ì„œ ì•™ìƒë¸”(PASS_3/PASS_2/FAIL/MISS)ì„ ìƒì„±í•˜ëŠ” ë¹„ë™ê¸° ë£¨í”„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\n\n"
        "- ì¦‰ì‹œ run_idë¥¼ ë°˜í™˜í•˜ë©°, ì§„í–‰ìƒí™©ì€ /loop/infer/status/{run_id}ì—ì„œ ì¡°íšŒí•©ë‹ˆë‹¤.\n"
        "- user_id/project_id ì œê³µ ì‹œ ëª¨ë¸ ì»¨í…Œì´ë„ˆì—ì„œ í”„ë¡œì íŠ¸ ìŠ¤ì½”í”„ weight ì„ íƒì— í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
    ),
    responses={
        200: {"description": "ë£¨í”„ê°€ ì •ìƒì ìœ¼ë¡œ ì‹œì‘ë¨"},
        400: {"description": "ìš”ì²­ íŒŒë¼ë¯¸í„° ì˜¤ë¥˜"},
        500: {"description": "ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜"},
    },
)
def loop_infer_run(
    req: InferRunRequest = Body(
        ...,
        openapi_examples={
            "basic": {
                "summary": "ê¸°ë³¸ ì‹¤í–‰",
                "value": {
                    "unlabeled_dir": "/workspace/storage/datasets/unlabeled/images",
                    "batch_size": 8,
                    "segment_size_batches": 100,
                    "infer_params": {"conf": 0.25, "iou": 0.45, "imgsz": 640, "device": "0"},
                },
            },
            "scoped": {
                "summary": "í”„ë¡œì íŠ¸ ìŠ¤ì½”í”„ (user_id/project_id í¬í•¨)",
                "value": {
                    "user_id": "user_001",
                    "project_id": "project_demo",
                    "unlabeled_dir": "/workspace/storage/datasets/project_demo/unlabeled/images",
                    "batch_size": 16,
                    "segment_size_batches": 50,
                    "infer_params": {"conf": 0.35, "imgsz": 640, "device": "0"},
                },
            },
        },
    ),
    background: BackgroundTasks = None,  # FastAPI ì£¼ì…ìš© (íƒ€ì…íŒíŠ¸ ìœ ì§€)
):
    run_id = req.run_id or time.strftime("run_%Y%m%d_%H%M%S")
    req = req.model_copy(update={"run_id": run_id})

    # BackgroundTasksëŠ” FastAPIê°€ ì£¼ì…í•˜ë¯€ë¡œ None ë°©ì–´
    if background is None:
        raise HTTPException(status_code=500, detail="BackgroundTasks injection failed")

    background.add_task(_do_infer, req)
    return InferRunResponse(run_id=run_id, status="RUNNING")


@app.get(
    "/loop/infer/status/{run_id}",
    tags=["Loop: Inference"],
    summary="ì¶”ë¡  ë£¨í”„ ìƒíƒœ ì¡°íšŒ",
    description="run_id ê¸°ì¤€ìœ¼ë¡œ í˜„ì¬ ì§„í–‰ìƒíƒœ(RUNNING/DONE/FAIL ë“±) ë° ì²˜ë¦¬ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.",
    responses={200: {"description": "ìƒíƒœ ì¡°íšŒ ì„±ê³µ"}, 404: {"description": "run_id ìƒíƒœ íŒŒì¼ì´ ì—†ìŒ(ë¯¸ì‹¤í–‰/ë§Œë£Œ)"}},
)
def loop_infer_status(
    run_id: str = FPath(..., description="Infer ë£¨í”„ ì‹¤í–‰ ì‹ë³„ì (/loop/infer/run ì‘ë‹µì˜ run_id)"),
):
    return read_status(run_id)


# -----------------------------------------------------------------------------
# ğŸ”¥ Train â€“ Judge Orchestration
# -----------------------------------------------------------------------------
def _call_train(model: str, req: GTTrainRequest) -> Dict[str, Any]:
    cfg, _ = _load_models_cfg_with_path()
    m = cfg["models"].get(model)
    if not m:
        raise HTTPException(status_code=400, detail=f"unknown model: {model}")

    base = m["base_url"].rstrip("/")
    ep = m.get("endpoints", {}).get("train_gt", "/train/gt")
    url = f"{base}{ep}"

    # âš ï¸ timeout=10ì€ í•™ìŠµ ìì²´ê°€ ì•„ë‹ˆë¼ "í•™ìŠµ íŠ¸ë¦¬ê±° ìš”ì²­" ì™•ë³µë§Œ ì»¤ë²„
    r = requests.post(url, json=req.model_dump(), timeout=10)
    r.raise_for_status()
    return r.json()


@app.post(
    "/loop/train/gt/run",
    response_model=GTTrainResponse,
    tags=["Loop: Train GT"],
    summary="GT í•™ìŠµ ì‹¤í–‰ (ë‹¨ì¼ ëª¨ë¸)",
    description=(
        "ìš”ì²­ì˜ req.modelì— í•´ë‹¹í•˜ëŠ” ëª¨ë¸ ì»¨í…Œì´ë„ˆë¡œ /train/gt ìš”ì²­ì„ ì „ë‹¬í•˜ì—¬ GT í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.\n\n"
        "- ì‹¤ì œ í•™ìŠµì€ ëª¨ë¸ ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì§„í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
        "- í•™ìŠµ ìƒíƒœëŠ” /loop/train/status/{model}/{train_job_id} ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤."
    ),
)
def loop_train_gt_run(
    req: GTTrainRequest = Body(
        ...,
        openapi_examples={
            "yolo_train": {
                "summary": "YOLOv11 GT í•™ìŠµ ì˜ˆì‹œ",
                "value": {
                    "identity": {"user_key": "user_001", "project_id": "project_demo", "job_id": "job_001"},
                    "model": "yolov11",
                    "dataset": {"format": "yolo", "img_root": "/workspace/storage/datasets/gt", "train": "labels/train"},
                    "train": {"epochs": 30, "imgsz": 640, "batch": 8, "device": "0", "extra": {}},
                    "init_weight_type": "baseline",
                },
            }
        },
    )
):
    return _call_train(req.model, req)


@app.post(
    "/loop/train/gt/run_all",
    tags=["Loop: Train GT"],
    summary="GT í•™ìŠµ ì‹¤í–‰ (3ëª¨ë¸ ì „ì²´)",
    description=(
        "YOLOv11/RTM/RT-DETR 3ê°œ ëª¨ë¸ì— ëŒ€í•´ GT í•™ìŠµ íŠ¸ë¦¬ê±°ë¥¼ ìˆœì°¨ í˜¸ì¶œí•©ë‹ˆë‹¤.\n"
        "- ì¼ë¶€ ëª¨ë¸ì´ ì‹¤íŒ¨í•´ë„ ë‚˜ë¨¸ì§€ ê²°ê³¼ëŠ” resultsì— ë‹´ê³ , failuresëŠ” errorsì— ë‹´ì•„ ë°˜í™˜í•©ë‹ˆë‹¤."
    ),
)
def loop_train_gt_run_all(
    req: GTTrainRequest = Body(
        ...,
        openapi_examples={
            "run_all": {
                "summary": "3ëª¨ë¸ ì „ì²´ GT í•™ìŠµ ì˜ˆì‹œ",
                "value": {
                    "identity": {"user_key": "user_001", "project_id": "project_demo", "job_id": "job_all_001"},
                    "model": "yolov11",
                    "dataset": {"format": "yolo", "img_root": "/workspace/storage/datasets/gt", "train": "labels/train"},
                    "train": {"epochs": 30, "imgsz": 640, "batch": 8, "device": "0", "extra": {}},
                    "init_weight_type": "baseline",
                },
            }
        },
    )
):
    results: Dict[str, Any] = {}
    errors: Dict[str, str] = {}

    for model in ("yolov11", "rtm", "rtdetr"):
        try:
            model_req = req.model_copy(update={"model": model})
            results[model] = _call_train(model, model_req)
        except Exception as e:
            errors[model] = f"{type(e).__name__}: {e}"

    if errors:
        return {"ok": False, "results": results, "errors": errors}

    return {"ok": True, "results": results}


@app.get(
    "/loop/train/status/{model}/{train_job_id}",
    tags=["Loop: Train GT"],
    summary="GT í•™ìŠµ ìƒíƒœ ì¡°íšŒ",
    description=(
        "ëª¨ë¸ ì»¨í…Œì´ë„ˆì˜ train status endpointë¡œ í”„ë¡ì‹œí•˜ì—¬ í•™ìŠµ ìƒíƒœë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\n"
        "- models.yamlì˜ models.{model}.endpoints.train_status ê°’ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n"
        "- ê¸°ë³¸ê°’ì€ /train/status/{train_job_id} í˜•íƒœë¥¼ ê°€ì •í•©ë‹ˆë‹¤."
    ),
)
def loop_train_gt_status(
    model: str = FPath(..., description="ëŒ€ìƒ ëª¨ë¸ ì´ë¦„ (yolov11 | rtm | rtdetr)"),
    train_job_id: str = FPath(..., description="ëª¨ë¸ ì»¨í…Œì´ë„ˆê°€ ë°œê¸‰/ì‚¬ìš©í•˜ëŠ” í•™ìŠµ job id"),
    timeout_s: float = Query(5.0, description="ìƒíƒœ ì¡°íšŒ ìš”ì²­ timeout(ì´ˆ). ëª¨ë¸ ì»¨í…Œì´ë„ˆê°€ ì‘ë‹µì´ ëŠë¦´ ë•Œ ì¡°ì •"),
):
    cfg, _ = _load_models_cfg_with_path()
    m = cfg["models"].get(model)
    if not m:
        raise HTTPException(status_code=400, detail=f"unknown model: {model}")

    base = m["base_url"].rstrip("/")
    ep = m.get("endpoints", {}).get("train_status", "/train/status")
    url = f"{base}{ep}/{train_job_id}"

    r = requests.get(url, timeout=timeout_s)
    r.raise_for_status()
    return r.json()
