#!/usr/bin/env python3
"""
End-to-end (NO HTTP) test:
model raw outputs (3 files) -> GatewayBundle(json) -> ensemble_bundle(real) -> merge 2 batches -> export.

Inputs (generated by gen_model_outputs_2batches.py):
  infer_req_b00.json, infer_req_b01.json
  yolov11_raw_b00.json, rtm_raw_b00.json, rtdetr_raw_b00.json
  yolov11_raw_b01.json, rtm_raw_b01.json, rtdetr_raw_b01.json
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List, Tuple

from src.ensemble.core import ensemble_bundle
from src.ensemble.export import export_yolo_txt, export_coco_json, export_meta_json


def read_json(p: Path) -> Any:
    return json.loads(p.read_text(encoding="utf-8"))


def write_json(p: Path, obj: Any) -> None:
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding="utf-8")


def gateway_bundle_from_raw(
    *,
    infer_req: Dict[str, Any],
    y_raw: Dict[str, Any],
    r_raw: Dict[str, Any],
    d_raw: Dict[str, Any],
) -> Dict[str, Any]:
    """
    Build GatewayBundle that core.py expects:
      {batch_id, items, responses:{model:ModelPredResponse}, errors:{}}

    ModelPredResponse:
      {model, batch_id, ok, error, elapsed_ms, weight_used, results, raw}
    """
    batch_id = str(infer_req["batch_id"])
    items = infer_req["items"]

    def wrap(model: str, raw: Dict[str, Any]) -> Dict[str, Any]:
        return {
            "model": model,
            "batch_id": batch_id,
            "ok": bool(raw.get("ok", True)),
            "error": raw.get("error"),
            "elapsed_ms": raw.get("elapsed_ms"),
            "weight_used": raw.get("weight_used"),
            "results": raw.get("results", []),
            "raw": raw,
        }

    return {
        "batch_id": batch_id,
        "items": items,
        "responses": {
            "yolov11": wrap("yolov11", y_raw),
            "rtm": wrap("rtm", r_raw),
            "rtdetr": wrap("rtdetr", d_raw),
        },
        "errors": {},
    }


def count_verdicts(verdicts: List[Dict[str, Any]]) -> Dict[str, int]:
    out: Dict[str, int] = {}
    for v in verdicts:
        k = str(v.get("verdict", ""))
        out[k] = out.get(k, 0) + 1
    return out


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument("--in-dir", type=str, required=True)
    ap.add_argument("--out-dir", type=str, default="./_test_runs/gateway_to_ensemble")
    ap.add_argument("--export-all", action="store_true", help="export all PASS images; default exports only first verdict")
    ap.add_argument("--w", type=int, default=1280)
    ap.add_argument("--h", type=int, default=720)
    args = ap.parse_args()

    in_dir = Path(args.in_dir).resolve()
    out_dir = Path(args.out_dir).resolve()
    out_dir.mkdir(parents=True, exist_ok=True)

    # load
    infer_b00 = read_json(in_dir / "infer_req_b00.json")
    infer_b01 = read_json(in_dir / "infer_req_b01.json")

    y00 = read_json(in_dir / "yolov11_raw_b00.json")
    r00 = read_json(in_dir / "rtm_raw_b00.json")
    d00 = read_json(in_dir / "rtdetr_raw_b00.json")

    y01 = read_json(in_dir / "yolov11_raw_b01.json")
    r01 = read_json(in_dir / "rtm_raw_b01.json")
    d01 = read_json(in_dir / "rtdetr_raw_b01.json")

    # gateway bundles
    b00 = gateway_bundle_from_raw(infer_req=infer_b00, y_raw=y00, r_raw=r00, d_raw=d00)
    b01 = gateway_bundle_from_raw(infer_req=infer_b01, y_raw=y01, r_raw=r01, d_raw=d01)

    write_json(out_dir / "bundle_b00.json", b00)
    write_json(out_dir / "bundle_b01.json", b01)

    # ensemble (real)
    v00, a00 = ensemble_bundle(b00)
    v01, a01 = ensemble_bundle(b01)

    write_json(out_dir / "verdicts_b00.json", v00)
    write_json(out_dir / "attributions_b00.json", a00)
    write_json(out_dir / "verdicts_b01.json", v01)
    write_json(out_dir / "attributions_b01.json", a01)

    merged_v = v00 + v01
    merged_a = a00 + a01
    write_json(out_dir / "verdicts_merged.json", merged_v)
    write_json(out_dir / "attributions_merged.json", merged_a)

    summary = {
        "n_b00": len(v00),
        "n_b01": len(v01),
        "n_merged": len(merged_v),
        "counts_b00": count_verdicts(v00),
        "counts_b01": count_verdicts(v01),
        "counts_merged": count_verdicts(merged_v),
        "n_attributions_merged": len(merged_a),
    }
    write_json(out_dir / "run_summary.json", summary)

    print("[ok] summary:", summary)

    # export policy: PASS_2/PASS_3 only gets yolo/coco, FAIL/MISS only meta
    wh = (int(args.w), int(args.h))
    exp_dir = out_dir / "export"
    labels_dir = exp_dir / "labels"
    ann_dir = exp_dir / "annotations"
    meta_dir = exp_dir / "meta"

    to_export = merged_v if args.export_all else [merged_v[0]]
    for v in to_export:
        export_yolo_txt(v, out_dir=labels_dir, image_wh=wh)
        export_coco_json(v, out_dir=ann_dir, image_wh=wh)
        export_meta_json(v, out_dir=meta_dir, image_wh=wh)

    print(f"[ok] export done. export_all={bool(args.export_all)} exported={len(to_export)}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
